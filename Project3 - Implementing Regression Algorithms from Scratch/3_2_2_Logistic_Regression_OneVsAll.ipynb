{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.2 Logistic Regression - Multiclass Classification - One Vs All Implementation\n",
    "\n",
    "### Challenge 3: [Implementating Regression Algorithms from Scratch]\n",
    "\n",
    "This project is a part of #100MLProjects\n",
    "\n",
    "One vs All is an approach where Logistic regression can be adopted to classify different classes, by default Logistic regression does binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('iris.data')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DR.VALLIAMMAI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\DR.VALLIAMMAI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0]\n",
      " [ 0 14  2]\n",
      " [ 0  0 13]]\n",
      "0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One vs All LogisticRegression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOvALinearRegression:\n",
    "  \"\"\" Custom Linear Regression Implementation.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  eta : float\n",
    "    Learning rate (between 0.0 and 1.0)\n",
    "  n_iter : int\n",
    "    Number of Epochs or Passes over the training set\n",
    "  random_state : int\n",
    "    Random number generator seed for random weight initialization.\n",
    "  \n",
    "  Attributes:\n",
    "  -----------\n",
    "  w_array_ : 1D Array\n",
    "    Array of Bias and Coefficients.\n",
    "    First element of w_ is the bias, and rest are the coefficients.\n",
    "  \"\"\"\n",
    "  def __init__(self, eta=0.03, n_iter=3000, random_state=123):\n",
    "    self.eta = eta\n",
    "    self.n_iter = n_iter\n",
    "    self.random_state = random_state\n",
    "    self.w_array_ = []\n",
    "\n",
    "    \n",
    "  def sigmoid(self, z):\n",
    "    \"\"\"Compute Sigmoid.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : {array-like}, shape=[n_samples, n_features]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sigmoid_value : float\n",
    "      returns the sigmoid value for given input.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "  def cost_function(self, y_hat, y):\n",
    "    \"\"\" Compute the Cost\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_hat : Predicted value\n",
    "    y     : Ground truth value\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    cost : Cost value\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    cost = (1/n)*(np.sum( np.dot(-y.T, np.log(y_hat)) - np.dot((1-y).T, np.log(1-y_hat)) )) \n",
    "    return cost\n",
    "    \n",
    "  def fit(self, X, y):\n",
    "    \"\"\"Fit training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : {array-like}, shape=[n_samples, n_features]\n",
    "      Training vectors where n_samples is the number of datapoints,\n",
    "      and n_features is the number of features.\n",
    "    y : array_like, shape=[n_samples]\n",
    "      Target Values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    self : object\n",
    "    \"\"\"\n",
    "    np.random.seed(self.random_state)\n",
    "\n",
    "    X_mod = np.insert(X, 0, 1, axis=1)\n",
    "    for target in np.unique(y):\n",
    "        y_mod = np.where(y == target, 1, 0)\n",
    "        w_ = np.random.rand(X_mod.shape[1])\n",
    "        w_, cost = self.gradient_descent(X_mod, y_mod, w_)\n",
    "        self.w_array_.append((w_, target))        \n",
    "    return self\n",
    "\n",
    "  def gradient_descent(self, X, y, w):\n",
    "    \"\"\"Compute Gradient Descent.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : {array-like}, shape=[n_samples, n_features+1]\n",
    "      Training vectors where n_samples is the number of datapoints,\n",
    "      and n_features is the number of features.\n",
    "    y : array_like, shape=[n_samples]\n",
    "      Target Values\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    w : {array-like}, shape=[n_features + 1]\n",
    "      optimized coefficients with bias unit.\n",
    "    cost: float\n",
    "      real number that quantifies the error.\n",
    "    \"\"\"\n",
    "    n = y.size\n",
    "    for _ in range(self.n_iter):\n",
    "      y_pred = np.dot(X, w)\n",
    "      error = y_pred - y\n",
    "      cost = (1/(2*n)) * np.dot(error.T, error)\n",
    "      w = w - (self.eta * (1/n) * np.dot(X.T, error))\n",
    "    return w, cost\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"Make predictions for new datapoint.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : {array-like}, shape=[n_samples, n_features]\n",
    "      Training vectors where n_samples is the number of datapoints,\n",
    "      and n_features is the number of features.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    y_pred: {array-like}, shape=[n_samples]\n",
    "      returns predicted continuous value.\n",
    "    \"\"\"\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    y_pred = [max( (self.sigmoid(np.dot(xi, w_)), target) for w_,target in self.w_array_)[1] for xi in X]\n",
    "    print(y_pred)\n",
    "    return y_pred\n",
    "#     return [1 if i > 0.5 else 0 for i in self.sigmoid(y_pred)]\n",
    "\n",
    "#     pred = [max(self.sigmoid(i.dot(w_)) for w_ in self.w_array_) for i in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr = CustomOvALinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr = clr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 1, 0, 2, 2, 1, 1, 2, 0, 0, 1, 0, 0, 1, 2, 1, 0, 0, 0, 0, 2, 0, 1, 1, 2, 0, 0, 2, 0, 1, 1, 2, 0, 2, 2, 1, 1, 0, 2, 0, 1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "pred = clr.predict(X_test)\n",
    "\n",
    "# print(confusion_matrix(y_test, pred))\n",
    "# print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[16  0  0]\n",
      " [ 0 12  4]\n",
      " [ 0  3 10]]\n",
      "\n",
      "Accuracy Score: \n",
      "0.8444444444444444\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix: \")\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(\"\\nAccuracy Score: \")\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "I have built a multi class classifier using logistic regression, but it is not as efficient as sklearns implementation, but it was great to learn how to implement logistic regression for multi class classification.\n",
    "\n",
    "I used the following references:\n",
    "\n",
    "- Machine Learning with Python book by sebastian raschska, a very good book.\n",
    "- [Logistic regression multiclass classification - towards datascience](https://medium.com/analytics-vidhya/logistic-regression-from-scratch-multi-classification-with-onevsall-d5c2acf0c37c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges Faced:\n",
    "\n",
    "After reading the theory and trying to implement this multiclass classification, I was able to build upto the step where i calculate the sigmoid score for each datapoint, and find the max value for it, but i didnt design it properly as i didnt have the associated label.\n",
    "\n",
    "fixing that is just a simple logical problem, but i was pressuring myself to find a clean solution. I later found an article that i quoted in the resources section above. I'm not satisfied with that solution too, but its simple and works fine. The solution requires us to append the label along with the coefficients.\n",
    "```\n",
    "self.w_array_.append((w_, target)) \n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
